{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnet Model and Training\n",
    "\n",
    "This aim of notebook is to define the siamese model that will be used for the Humpback Whale challenge. It will also run the model to trainin it agains our previously generated training and validation dataset and also create the results agains a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Module Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training, Validation and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31218, 44, 100) (31218, 44, 100) (31218,)\n",
      "Validation set (5510, 44, 100) (5510, 44, 100) (5510,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../data/Siamese_dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_X1_dataset = save['train_X1_dataset']\n",
    "    train_X2_dataset = save['train_X2_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_X1_dataset = save['valid_X1_dataset']\n",
    "    valid_X2_dataset = save['valid_X2_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_X1_dataset.shape, train_X2_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_X1_dataset.shape, valid_X2_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Declaration of Global Constants\n",
    "Now we declare all the global constants in hee in orde to have them centralized.\n",
    "This way we can easyly tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image parameters\n",
    "NUM_CHANNELS = 1\n",
    "IMAGE_HEIGHT = 44\n",
    "IMAGE_WIDHT = 100\n",
    "\n",
    "#Convnet parameters\n",
    "BETA = 0.005\n",
    "DEPTH_1 = 8\n",
    "\n",
    "#Training parameters\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reformat datasets into a TensorFlow-friendly shape:\n",
    "\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to reformat ?? TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definition of auxiliar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Accuracy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Network declaration\n",
    "We will use tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-85cedc4c327b>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-85cedc4c327b>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    'h1' :  tf.Variable(tf.zeros([depth]))\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "siamese_model = tf.Graph()\n",
    "\n",
    "with siamese_model.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "      tf.float32, shape=(batch_size, IMAGE_WIDHT, IMAGE_HEIGHT, NUM_CHANNELS))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(BATCH_SIZE, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "    weights = {\n",
    "      'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden1_nodes]) ),\n",
    "      'out': tf.Variable( tf.truncated_normal([hidden1_nodes, num_labels]) )\n",
    "      }\n",
    "    biases = {\n",
    "        'h1' :  tf.Variable(tf.zeros([depth])),\n",
    "        'out' :  tf.Variable(tf.zeros([depth]))\n",
    "        }\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
