{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnet Model and Training\n",
    "\n",
    "This aim of notebook is to define the siamese model that will be used for the Humpback Whale challenge. It will also run the model to trainin it agains our previously generated training and validation dataset and also create the results agains a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Module Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training, Validation and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (15607, 44, 82) (15607, 44, 82) (15607,)\n",
      "Validation set (2755, 44, 82) (2755, 44, 82) (2755,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../data/Siamese_dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_X1_dataset = save['train_X1_dataset']\n",
    "    train_X2_dataset = save['train_X2_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_X1_dataset = save['valid_X1_dataset']\n",
    "    valid_X2_dataset = save['valid_X2_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_X1_dataset.shape, train_X2_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_X1_dataset.shape, valid_X2_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Declaration of Global Constants\n",
    "Now we declare all the global constants in hee in orde to have them centralized.\n",
    "This way we can easyly tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image parameters\n",
    "NUM_CHANNELS = 1\n",
    "IMAGE_HEIGHT = 44\n",
    "IMAGE_WIDHT = 82\n",
    "\n",
    "#Convnet parameters\n",
    "BETA = 0.005\n",
    "PATCH_1 = 7\n",
    "PATCH_2 = 6\n",
    "DEPTH_1 = 15\n",
    "DEPTH_2 = 45\n",
    "NUM_HIDEN = 1024\n",
    "NUM_LABELS = 1\n",
    "MARGIN = 8.0\n",
    "\n",
    "\n",
    "#Training parameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_STEPS = 1501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reformat datasets into a TensorFlow-friendly shape:\n",
    "\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (15607, 44, 82, 1) (15607, 44, 82, 1) (15607, 1)\n",
      "Validation set (2755, 44, 82, 1) (2755, 44, 82, 1) (2755, 1)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 1\n",
    "\n",
    "def reformat(dataset1, dataset2, labels):\n",
    "  dataset1 = dataset1.reshape(\n",
    "    (-1, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS)).astype(np.float32)\n",
    "  dataset2 = dataset2.reshape(\n",
    "    (-1, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset1, dataset2, labels\n",
    "train_X1_dataset, train_X2_dataset, train_labels = reformat(train_X1_dataset, train_X2_dataset, train_labels)\n",
    "valid_X1_dataset, valid_X2_dataset, valid_labels = reformat(valid_X1_dataset, valid_X2_dataset, valid_labels)\n",
    "print('Training set', train_X1_dataset.shape, train_X2_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_X1_dataset.shape, valid_X2_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definition of auxiliar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Accuracy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    aux = predictions < MARGIN\n",
    "    equal = np.equal(aux[:] , labels[:, 0])\n",
    "    great = np.greater(aux[:] , labels[:, 0])\n",
    "    less = np.less(aux[:] , labels[:, 0])\n",
    "    acc = (100.0 * np.sum(equal.astype(np.float32))\n",
    "          / predictions.shape[0])\n",
    "    false_pos = np.sum(great.astype(np.float32))\n",
    "    false_neg = np.sum(less.astype(np.float32))\n",
    "    return acc, false_pos, false_neg\n",
    "\n",
    "def print_means(distance, labels):\n",
    "    print(distance.shape)\n",
    "    print(labels.shape)\n",
    "    positives = np.where(labels[:, 0] > 0)\n",
    "    negatives = np.where(labels[:, 0] == 0)\n",
    "    print(positives[0].shape)\n",
    "    print(negatives[0].shape)\n",
    "    mean_p_distance = np.mean(distance[positives[0]])\n",
    "    mean_n_distance = np.mean(distance[negatives[0]])\n",
    "    print('Mean negative distance ' +  str(mean_n_distance) + ' mean positive distance ' + str(mean_p_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Network declaration\n",
    "We will use tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 11, 21, 45]\n",
      "[128, 11, 21, 45]\n",
      "[2755, 11, 21, 45]\n",
      "[2755, 11, 21, 45]\n",
      "(128,)\n",
      "(2755,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "siamese_model = tf.Graph()\n",
    "\n",
    "with siamese_model.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    tf_train_X1_dataset = tf.placeholder(\\\n",
    "          tf.float32, shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS))\n",
    "    tf_train_X2_dataset = tf.placeholder(\\\n",
    "          tf.float32, shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_LABELS))\n",
    "    tf_valid_X1_dataset = tf.constant(valid_X1_dataset)\n",
    "    tf_valid_X2_dataset = tf.constant(valid_X2_dataset)\n",
    "    #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = {\n",
    "      'layer1': tf.Variable(tf.truncated_normal(\\\n",
    "              [PATCH_1, PATCH_1, NUM_CHANNELS, DEPTH_1], stddev=0.1)),\n",
    "      'layer2': tf.Variable(tf.truncated_normal(\\\n",
    "              [PATCH_2, PATCH_2, DEPTH_1, DEPTH_2], stddev=0.1)),\n",
    "      'layer3': tf.Variable(tf.truncated_normal(\\\n",
    "              [IMAGE_HEIGHT // 4 * (IMAGE_WIDHT // 4 + 1) * DEPTH_2, NUM_HIDEN], stddev=0.1)),\n",
    "      'layer4': tf.Variable(tf.truncated_normal(\\\n",
    "              [NUM_HIDEN, 2], stddev=0.1))\n",
    "      }\n",
    "    biases = {\n",
    "        'layer1' : tf.Variable(tf.zeros([DEPTH_1])),\n",
    "        'layer2' : tf.Variable(tf.constant(1.0, shape=[DEPTH_2])),\n",
    "        'layer3' : tf.Variable(tf.constant(1.0, shape=[NUM_HIDEN])),\n",
    "        'layer4' : tf.Variable(tf.constant(1.0, shape=[1]))\n",
    "        }\n",
    "  \n",
    "  # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, weights['layer1'], [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['layer1'])\n",
    "        conv = tf.nn.conv2d(hidden, weights['layer2'], [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['layer2'])\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        print(shape)\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, weights['layer3']) + biases['layer3'])\n",
    "        return tf.matmul(hidden, weights['layer4']) + biases['layer4']\n",
    "    \n",
    "    \n",
    "  # Training computation.\n",
    "    with tf.variable_scope(\"siamese\") as scope:\n",
    "        logits1 = model(tf_train_X1_dataset)\n",
    "        scope.reuse_variables()\n",
    "        logits2 = model(tf_train_X2_dataset)\n",
    "        \n",
    "    with tf.variable_scope(\"siamese_val\") as scope:\n",
    "        val_logits1 = model(tf_valid_X1_dataset)\n",
    "        scope.reuse_variables()\n",
    "        val_logits2 = model(tf_valid_X2_dataset)\n",
    "        \n",
    "    def loss(y):\n",
    "        labels_t = y\n",
    "        labels_f = tf.subtract(1.0, y, name=\"1-yi\")          # labels_ = !labels;\n",
    "        eucd2 = tf.pow(tf.subtract(logits1, logits2), 2)\n",
    "        eucd2 = tf.reduce_sum(eucd2, 1)\n",
    "        eucd = tf.sqrt(eucd2+1e-6, name=\"eucd\")\n",
    "        C = tf.constant(MARGIN, name=\"C\")\n",
    "        # yi*||CNN(p1i)-CNN(p2i)||^2 + (1-yi)*max(0, C-||CNN(p1i)-CNN(p2i)||^2)\n",
    "        pos = tf.multiply(labels_t, eucd2, name=\"yi_x_eucd2\")\n",
    "        # neg = tf.multiply(labels_f, tf.subtract(0.0,eucd2), name=\"yi_x_eucd2\")\n",
    "        # neg = tf.multiply(labels_f, tf.maximum(0.0, tf.subtract(C,eucd2)), name=\"Nyi_x_C-eucd_xx_2\")\n",
    "        neg = tf.multiply(labels_f, tf.pow(tf.maximum(tf.subtract(C, eucd), 0), 2), name=\"Nyi_x_C-eucd_xx_2\")\n",
    "        losses = tf.add(pos, neg, name=\"losses\")\n",
    "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
    "        return loss\n",
    "        \n",
    "    loss = loss(tf_train_labels)\n",
    "    \n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "  \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.reduce_sum(tf.pow(tf.subtract(logits1, logits2), 2),1)\n",
    "    print(train_prediction.shape)\n",
    "    valid_prediction = tf.reduce_sum(tf.pow(tf.subtract(val_logits1, val_logits2), 2),1)\n",
    "    print(valid_prediction.shape)\n",
    "    #test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 28.7582817078\n",
      "Minibatch accuracy: 31.25 false positives: 36.0 false negatives: 52.0\n",
      "Validation accuracy: 42.5045372051 false positives: 197.0 false negatives: 1387.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 482.16696 mean positive distance 1702.8926\n",
      "Minibatch loss at step 50 19.1723957062\n",
      "Minibatch accuracy: 40.625 false positives: 41.0 false negatives: 35.0\n",
      "Validation accuracy: 45.55353902 false positives: 795.0 false negatives: 705.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 10.6664715 mean positive distance 12.554004\n",
      "Minibatch loss at step 100 20.4884872437\n",
      "Minibatch accuracy: 44.53125 false positives: 39.0 false negatives: 32.0\n",
      "Validation accuracy: 41.8511796733 false positives: 784.0 false negatives: 818.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 10.817733 mean positive distance 14.938953\n",
      "Minibatch loss at step 150 18.6811771393\n",
      "Minibatch accuracy: 42.96875 false positives: 34.0 false negatives: 39.0\n",
      "Validation accuracy: 44.3920145191 false positives: 769.0 false negatives: 763.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 11.635892 mean positive distance 13.573081\n",
      "Minibatch loss at step 200 19.5382442474\n",
      "Minibatch accuracy: 51.5625 false positives: 29.0 false negatives: 33.0\n",
      "Validation accuracy: 44.5735027223 false positives: 751.0 false negatives: 776.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 11.630278 mean positive distance 13.986008\n",
      "Minibatch loss at step 250 20.4063205719\n",
      "Minibatch accuracy: 43.75 false positives: 35.0 false negatives: 37.0\n",
      "Validation accuracy: 42.722323049 false positives: 746.0 false negatives: 832.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 11.572523 mean positive distance 15.213344\n",
      "Minibatch loss at step 300 19.5571746826\n",
      "Minibatch accuracy: 39.0625 false positives: 32.0 false negatives: 46.0\n",
      "Validation accuracy: 44.8275862069 false positives: 677.0 false negatives: 843.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 12.779713 mean positive distance 16.453875\n",
      "Minibatch loss at step 350 19.4500179291\n",
      "Minibatch accuracy: 45.3125 false positives: 30.0 false negatives: 40.0\n",
      "Validation accuracy: 43.5208711434 false positives: 724.0 false negatives: 832.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 12.219429 mean positive distance 16.292824\n",
      "Minibatch loss at step 400 19.2683582306\n",
      "Minibatch accuracy: 37.5 false positives: 32.0 false negatives: 48.0\n",
      "Validation accuracy: 43.5934664247 false positives: 690.0 false negatives: 864.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 13.043368 mean positive distance 18.169643\n",
      "Minibatch loss at step 450 18.6883392334\n",
      "Minibatch accuracy: 49.21875 false positives: 31.0 false negatives: 34.0\n",
      "Validation accuracy: 45.2994555354 false positives: 748.0 false negatives: 759.0\n",
      "(2755,)\n",
      "(2755, 1)\n",
      "(1406,)\n",
      "(1349,)\n",
      "Mean negative distance 10.989254 mean positive distance 13.132971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d54c33208d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_X1_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data_X1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_X2_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data_X2\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     _, _loss, predictions = session.run(\n\u001b[0;32m---> 17\u001b[0;31m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Plots\n",
    "P_FREQ = 50\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "with tf.Session(graph=siamese_model) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(NUM_STEPS):\n",
    "    offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "    batch_data_X1 = train_X1_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_data_X2 = train_X2_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "    feed_dict = {tf_train_X1_dataset : batch_data_X1, tf_train_X2_dataset : batch_data_X2, \\\n",
    "                 tf_train_labels : batch_labels}\n",
    "    _, _loss, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if np.isnan(_loss):\n",
    "        print('Model diverged with loss = NaN')\n",
    "        quit()\n",
    "        \n",
    "    #Plot variables\n",
    "    if (step % P_FREQ == 0):\n",
    "      losses.append(_loss)\n",
    "      distances = valid_prediction.eval()\n",
    "      acc, fp, fn = accuracy(distances, valid_labels)\n",
    "      accuracies.append(acc)\n",
    "      rates.append(float(fp/fn))\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step {} {}'.format(step, _loss))\n",
    "      a = accuracy(predictions, batch_labels)\n",
    "      print('Minibatch accuracy: {} false positives: {} false negatives: {}'.format(a[0], a[1], a[2] ) )\n",
    "      a = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      print('Validation accuracy: {} false positives: {} false negatives: {}'.format(a[0], a[1], a[2] ) )\n",
    "      print_means(valid_prediction.eval(), valid_labels)\n",
    "#print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, NUM_STEPS, P_FREQ), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, NUM_STEPS, P_FREQ), accuracies)\n",
    "ax2.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, NUM_STEPS, P_FREQ), rates)\n",
    "ax3.set_ylabel(\"FP / FN [%]\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hey < 5.0\n",
    "print(a.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_valid_labels)\n",
    "print(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[23].astype(np.float32))\n",
    "print(valid_labels[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_dbg(predictions, labels, print_nr = 0):\n",
    "    aux = predictions < MARGIN\n",
    "    equal = np.equal(aux[:] , labels[:,0])\n",
    "    print(predictions[print_nr])\n",
    "    print(labels[print_nr])\n",
    "    print(aux[print_nr].astype(np.float32))\n",
    "    print(aux.shape)\n",
    "    print(labels.shape)\n",
    "    print(equal.shape)\n",
    "    print(predictions.shape)\n",
    "    acc = (100.0 * np.sum(equal.astype(np.float32))\n",
    "          / predictions.shape[0])\n",
    "    false_pos = 12\n",
    "    false_neg = 122\n",
    "    return acc, false_pos, false_neg\n",
    "a = accuracy_dbg(hey, valid_labels, 24)\n",
    "print(a)\n",
    "print(\" {} {} {}\".format(a[0], a[1], a[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1, 1, 1, 1, 0, 0, 0])\n",
    "b = np.where(a == 0)\n",
    "c = a[b]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
