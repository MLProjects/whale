{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnet Model and Training\n",
    "\n",
    "This aim of notebook is to define the siamese model that will be used for the Humpback Whale challenge. It will also run the model to trainin it agains our previously generated training and validation dataset and also create the results agains a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Module Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training, Validation and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31218, 44, 100) (31218, 44, 100) (31218,)\n",
      "Validation set (5510, 44, 100) (5510, 44, 100) (5510,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../data/Siamese_dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_X1_dataset = save['train_X1_dataset']\n",
    "    train_X2_dataset = save['train_X2_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_X1_dataset = save['valid_X1_dataset']\n",
    "    valid_X2_dataset = save['valid_X2_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_X1_dataset.shape, train_X2_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_X1_dataset.shape, valid_X2_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Declaration of Global Constants\n",
    "Now we declare all the global constants in hee in orde to have them centralized.\n",
    "This way we can easyly tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image parameters\n",
    "NUM_CHANNELS = 1\n",
    "IMAGE_HEIGHT = 44\n",
    "IMAGE_WIDHT = 100\n",
    "\n",
    "#Convnet parameters\n",
    "BETA = 0.005\n",
    "PATCH_1 = 5\n",
    "PATCH_2 = 5\n",
    "DEPTH_1 = 8\n",
    "DEPTH_2 = 15\n",
    "NUM_HIDEN = 1024\n",
    "NUM_LABELS = 1\n",
    "MARGIN = 5.0\n",
    "\n",
    "\n",
    "#Training parameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_STEPS = 1501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reformat datasets into a TensorFlow-friendly shape:\n",
    "\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31218, 44, 100, 1) (31218, 44, 100, 1) (31218, 1, 1)\n",
      "Validation set (5510, 44, 100, 1) (5510, 44, 100, 1) (5510, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 1\n",
    "\n",
    "def reformat(dataset1, dataset2, labels):\n",
    "  dataset1 = dataset1.reshape(\n",
    "    (-1, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS)).astype(np.float32)\n",
    "  dataset2 = dataset2.reshape(\n",
    "    (-1, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset1, dataset2, labels\n",
    "train_X1_dataset, train_X2_dataset, train_labels = reformat(train_X1_dataset, train_X2_dataset, train_labels)\n",
    "valid_X1_dataset, valid_X2_dataset, valid_labels = reformat(valid_X1_dataset, valid_X2_dataset, valid_labels)\n",
    "print('Training set', train_X1_dataset.shape, train_X2_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_X1_dataset.shape, valid_X2_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definition of auxiliar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Accuracy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    aux = predictions < MARGIN\n",
    "    equal = np.equal(aux[:] , labels[:, 0])\n",
    "    return (100.0 * np.sum(equal.astype(np.float32))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Network declaration\n",
    "We will use tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(5510,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "siamese_model = tf.Graph()\n",
    "\n",
    "with siamese_model.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    tf_train_X1_dataset = tf.placeholder(\\\n",
    "          tf.float32, shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS))\n",
    "    tf_train_X2_dataset = tf.placeholder(\\\n",
    "          tf.float32, shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDHT, NUM_CHANNELS))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_LABELS))\n",
    "    tf_valid_X1_dataset = tf.constant(valid_X1_dataset)\n",
    "    tf_valid_X2_dataset = tf.constant(valid_X2_dataset)\n",
    "    #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = {\n",
    "      'layer1': tf.Variable(tf.truncated_normal(\\\n",
    "              [PATCH_1, PATCH_1, NUM_CHANNELS, DEPTH_1], stddev=0.1)),\n",
    "      'layer2': tf.Variable(tf.truncated_normal(\\\n",
    "              [PATCH_2, PATCH_2, DEPTH_1, DEPTH_2], stddev=0.1)),\n",
    "      'layer3': tf.Variable(tf.truncated_normal(\\\n",
    "              [IMAGE_HEIGHT // 4 * IMAGE_WIDHT // 4 * DEPTH_2, NUM_HIDEN], stddev=0.1)),\n",
    "      'layer4': tf.Variable(tf.truncated_normal(\\\n",
    "              [NUM_HIDEN, 10], stddev=0.1))\n",
    "      }\n",
    "    biases = {\n",
    "        'layer1' : tf.Variable(tf.zeros([DEPTH_1])),\n",
    "        'layer2' : tf.Variable(tf.constant(1.0, shape=[DEPTH_2])),\n",
    "        'layer3' : tf.Variable(tf.constant(1.0, shape=[NUM_HIDEN])),\n",
    "        'layer4' : tf.Variable(tf.constant(1.0, shape=[1]))\n",
    "        }\n",
    "  \n",
    "  # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, weights['layer1'], [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['layer1'])\n",
    "        conv = tf.nn.conv2d(hidden, weights['layer2'], [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['layer2'])\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, weights['layer3']) + biases['layer3'])\n",
    "        return tf.matmul(hidden, weights['layer4']) + biases['layer4']\n",
    "    \n",
    "    \n",
    "  # Training computation.\n",
    "    with tf.variable_scope(\"siamese\") as scope:\n",
    "        logits1 = model(tf_train_X1_dataset)\n",
    "        scope.reuse_variables()\n",
    "        logits2 = model(tf_train_X2_dataset)\n",
    "        \n",
    "    with tf.variable_scope(\"siamese_val\") as scope:\n",
    "        val_logits1 = model(tf_valid_X1_dataset)\n",
    "        scope.reuse_variables()\n",
    "        val_logits2 = model(tf_valid_X2_dataset)\n",
    "        \n",
    "    def loss(y):\n",
    "        labels_t = y\n",
    "        labels_f = tf.subtract(1.0, y, name=\"1-yi\")          # labels_ = !labels;\n",
    "        eucd2 = tf.pow(tf.subtract(logits1, logits2), 2)\n",
    "        eucd2 = tf.reduce_sum(eucd2, 1)\n",
    "        eucd = tf.sqrt(eucd2+1e-6, name=\"eucd\")\n",
    "        C = tf.constant(MARGIN, name=\"C\")\n",
    "        # yi*||CNN(p1i)-CNN(p2i)||^2 + (1-yi)*max(0, C-||CNN(p1i)-CNN(p2i)||^2)\n",
    "        pos = tf.multiply(labels_t, eucd2, name=\"yi_x_eucd2\")\n",
    "        # neg = tf.multiply(labels_f, tf.subtract(0.0,eucd2), name=\"yi_x_eucd2\")\n",
    "        # neg = tf.multiply(labels_f, tf.maximum(0.0, tf.subtract(C,eucd2)), name=\"Nyi_x_C-eucd_xx_2\")\n",
    "        neg = tf.multiply(labels_f, tf.pow(tf.maximum(tf.subtract(C, eucd), 0), 2), name=\"Nyi_x_C-eucd_xx_2\")\n",
    "        losses = tf.add(pos, neg, name=\"losses\")\n",
    "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
    "        return loss\n",
    "        \n",
    "    loss = loss(tf_train_labels)\n",
    "    \n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "  \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.reduce_sum(tf.pow(tf.subtract(logits1, logits2), 2),1)\n",
    "    print(train_prediction.shape)\n",
    "    valid_prediction = tf.reduce_sum(tf.pow(tf.subtract(val_logits1, val_logits2), 2),1)\n",
    "    print(valid_prediction.shape)\n",
    "    #test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128, 1, 1) for Tensor u'Placeholder_2:0', which has shape '(128, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-5eedc61f4fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_X1_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data_X1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_X2_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data_X2\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     _, _loss, predictions = session.run(\n\u001b[0;32m---> 17\u001b[0;31m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128, 1, 1) for Tensor u'Placeholder_2:0', which has shape '(128, 1)'"
     ]
    }
   ],
   "source": [
    "#Plots\n",
    "P_FREQ = 50\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "with tf.Session(graph=siamese_model) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(NUM_STEPS):\n",
    "    offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "    batch_data_X1 = train_X1_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_data_X2 = train_X2_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "    feed_dict = {tf_train_X1_dataset : batch_data_X1, tf_train_X2_dataset : batch_data_X2, \\\n",
    "                 tf_train_labels : batch_labels}\n",
    "    _, _loss, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if np.isnan(_loss):\n",
    "        print('Model diverged with loss = NaN')\n",
    "        quit()\n",
    "        \n",
    "    #Plot variables\n",
    "    if (step % P_FREQ == 0):\n",
    "      losses.append(_loss)\n",
    "      hey= valid_prediction.eval()\n",
    "      accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, _loss))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "#print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, NUM_STEPS, P_FREQ), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, NUM_STEPS, P_FREQ), accuracies)\n",
    "ax2.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5510, 1)\n"
     ]
    }
   ],
   "source": [
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.11904621  7.43290091  7.31785154 ...,  9.63284588  7.26450729\n",
      "  4.35797882]\n"
     ]
    }
   ],
   "source": [
    "print(hey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "a = hey < 5.0\n",
    "print(a.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  1.  0.  0.]\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(_valid_labels)\n",
    "print(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "print(a[23].astype(np.float32))\n",
    "print(valid_labels[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.54601\n",
      "[ 1.]\n",
      "0.0\n",
      "(5510,)\n",
      "(5510, 1)\n",
      "(5510,)\n",
      "(5510,)\n",
      "44.3194192377\n"
     ]
    }
   ],
   "source": [
    "def accuracy_dbg(predictions, labels, print_nr = 0):\n",
    "    aux = predictions < MARGIN\n",
    "    equal = np.equal(aux[:] , labels[:,0])\n",
    "    print(predictions[print_nr])\n",
    "    print(labels[print_nr])\n",
    "    print(aux[print_nr].astype(np.float32))\n",
    "    print(aux.shape)\n",
    "    print(labels.shape)\n",
    "    print(equal.shape)\n",
    "    print(predictions.shape)\n",
    "    return (100.0 * np.sum(equal.astype(np.float32))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "print(accuracy_dbg(hey, valid_labels, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
